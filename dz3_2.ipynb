{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUjockh6T5A2Qyeas6+W7c"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Spark Apache (семинары)\n",
        "\n",
        "###Урок 3. Инструменты работы и визуализации ч.2\n",
        "\n",
        "Условие: есть набор данных о продажах продуктов с информацией о дате продаж, категории продукта, количестве и выручке от продаж.\n",
        "\n",
        "Используя Apache Spark, загрузите предоставленный набор данных в DataFrame (пример данных ниже).\n",
        "\n",
        "(\"2023-11-20\", \"Electronics\", 100, 12000),\n",
        "(\"2023-11-21\", \"Electronics\", 110, 13000),\n",
        "(\"2023-11-22\", \"Electronics\", 105, 12500),\n",
        "(\"2023-11-20\", \"Clothing\", 300, 15000),\n",
        "(\"2023-11-21\", \"Clothing\", 280, 14000),\n",
        "(\"2023-11-22\", \"Clothing\", 320, 16000),\n",
        "(\"2023-11-20\", \"Books\", 150, 9000),\n",
        "(\"2023-11-21\", \"Books\", 200, 12000),\n",
        "(\"2023-11-22\", \"Books\", 180, 10000)\n",
        "\n",
        "Столбцы: \"date\", \"category\", \"quantity\", \"revenue\".\n",
        "\n",
        "С использованием оконных функций, рассчитайте среднее выручки от продаж для каждой категории продукта.\n",
        "Примените операцию pivot для того, чтобы преобразовать полученные данные таким образом, чтобы в качестве строк были категории продуктов, в качестве столбцов были дни, а значениями были средние значения выручки от продаж за соответствующий день"
      ],
      "metadata": {
        "id": "il0SuIC-J4zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "KS5zt87_Lc-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, mean, round\n",
        "from pyspark.sql.window import Window"
      ],
      "metadata": {
        "id": "QFls8nw8L1Ro"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pajcNicLJ4Va"
      },
      "outputs": [],
      "source": [
        "data = (\"2023-11-20\", \"Electronics\", 100, 12000), (\"2023-11-21\", \"Electronics\", 110, 13000), (\"2023-11-22\", \"Electronics\", 105, 12500), (\"2023-11-20\", \"Clothing\", 300, 15000), (\"2023-11-21\", \"Clothing\", 280, 14000), (\"2023-11-22\", \"Clothing\", 320, 16000), (\"2023-11-20\", \"Books\", 150, 9000), (\"2023-11-21\", \"Books\", 200, 12000), (\"2023-11-22\", \"Books\", 180, 10000)\n",
        "df = pd.DataFrame(data, columns=[\"date\", \"category\", \"quantity\", \"revenue\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('dz2').getOrCreate()\n",
        "dfs = spark.createDataFrame(df)\n",
        "# С использованием оконных функций, рассчитайте среднее выручки от продаж для каждой категории продукта\n",
        "dfs.withColumn('mean_revenue', mean(col(\"revenue\")*col('quantity')).over(Window.partitionBy('category'))).show()\n",
        "\n",
        "#Примените операцию pivot\n",
        "dfs.groupBy('category').pivot('date').agg(mean(col(\"revenue\")*col('quantity'))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC90pAkTMu26",
        "outputId": "debbc200-c22f-4172-e57c-b0bcacb06782"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+--------+-------+------------------+\n",
            "|      date|   category|quantity|revenue|      mean_revenue|\n",
            "+----------+-----------+--------+-------+------------------+\n",
            "|2023-11-20|      Books|     150|   9000|         1850000.0|\n",
            "|2023-11-21|      Books|     200|  12000|         1850000.0|\n",
            "|2023-11-22|      Books|     180|  10000|         1850000.0|\n",
            "|2023-11-20|   Clothing|     300|  15000| 4513333.333333333|\n",
            "|2023-11-21|   Clothing|     280|  14000| 4513333.333333333|\n",
            "|2023-11-22|   Clothing|     320|  16000| 4513333.333333333|\n",
            "|2023-11-20|Electronics|     100|  12000|1314166.6666666667|\n",
            "|2023-11-21|Electronics|     110|  13000|1314166.6666666667|\n",
            "|2023-11-22|Electronics|     105|  12500|1314166.6666666667|\n",
            "+----------+-----------+--------+-------+------------------+\n",
            "\n",
            "+-----------+----------+----------+----------+\n",
            "|   category|2023-11-20|2023-11-21|2023-11-22|\n",
            "+-----------+----------+----------+----------+\n",
            "|Electronics| 1200000.0| 1430000.0| 1312500.0|\n",
            "|   Clothing| 4500000.0| 3920000.0| 5120000.0|\n",
            "|      Books| 1350000.0| 2400000.0| 1800000.0|\n",
            "+-----------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "QmmvwkruL8wD"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}